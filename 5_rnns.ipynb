{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs para Martín Fierro\n",
    "\n",
    "El objetivo de los ejercicios en este tutorial es mostrar el impacto de algunas decisiones de diseño en la implementación de las redes neuronales, particularmente las recurrentes. Como ejemplo veremos una implementación de la red RNN para generación de lenguaje basada en caracteres de [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Para entrenarla utilizaremos un fragmento del Martín Fierro que pueden descargar [aquí](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt). Para un entrenamiento más complejo, pueden utilizar las obras completas de borges, disponibles en [este link](https://drive.google.com/file/d/0B4remi0ZCiqbUFpTS19pSmVFYkU/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leeremos el dataset del archivo de texto y lo preprocesaremos para disminuir la viariación de caracteres. Normalizaremos el formato unicos, elminaremos espacios y transformaremos todo a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-10-15 20:54:51--  https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt\n",
      "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
      "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35910 (35K) [text/plain]\n",
      "Saving to: ‘martin_fierro.txt’\n",
      "\n",
      "     0K .......... .......... .......... .....                100% 40.6M=0.001s\n",
      "\n",
      "2019-10-15 20:54:51 (40.6 MB/s) - ‘martin_fierro.txt’ saved [35910/35910]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 33858\n"
     ]
    }
   ],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, contaremos la cantidad de caracteres únicos presentes en el texto, y le asignaremos a cada uno un índice único y secuencial. Este índice será utilizado luego para crear las representaciones one-hot encoding de los caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 54\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print('Total chars: %d' % len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Esqueleto de la red neuronal\n",
    "\n",
    "Lo primero que debemos pensar es cómo será la arquitectura de nuestra red para resolver la tarea deseada. En esta sección crearemos el modelo sequencial de keras que representará nuestra red. En los pasos siguientes, implementaremos las transformaciones del corpus, por lo que en este paso pueden asumir cualquier formato en los datos de entrada.\n",
    "\n",
    "Para poder implementar el modelo debemos responder las siguientes preguntas:\n",
    "  - ¿Es una red one-to-one, one-to-many, many-to-one o many-to-many?\n",
    "  - ¿Cuál es el formato de entrada y de salida de la red? ¿Cuál es el tamaño de las matrices (tensores) de entrada y de salida?\n",
    "  - Luego de que la entrada pasa por la capa recurrente, ¿qué tamaño tiene el tensor?\n",
    "  - ¿Cómo se conecta la salida de la capa recurrente con la capa densa que realiza la clasificación?\n",
    "  - ¿Cuál es el loss apropiado para este problema?\n",
    "\n",
    "Las funciones de Keras que tendrán que utilizar son:\n",
    "  - keras.layers.LSTM\n",
    "  - keras.layers.TimeDistributed\n",
    "  - keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 40, 128)           93696     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 40, 54)            6966      \n",
      "=================================================================\n",
      "Total params: 100,662\n",
      "Trainable params: 100,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# build the model: a single LSTM\n",
    "model = tf.keras.Sequential()\n",
    "hidden_layer_size = 128\n",
    "maxlen = 40\n",
    "model.add(layers.LSTM(hidden_layer_size, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "# The output of the network at this point has shape (batch_size, maxlen, hidden_layer_size)\n",
    "# We need to convert it into something of shape (batch_size, maxlen, len(chars))\n",
    "# by applying THE SAME dense layer to all the times in the sequence.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Transformación del input\n",
    "\n",
    "Una vez que definimos la arquitectura de la red, sabemos con exactitud cuál es el input que necesitamos utilizar. En esta sección transformaremos el texto que leimos del archivo en ejemplos de entrenamiento para nuestra red. El resultado será una matrix que representa las secuencias de caracteres y una matriz que representa las etiquetas correspondientes.\n",
    "\n",
    "  - ¿Cómo debemos representar cada ejemplo?\n",
    "  - ¿Cómo debemos representar cada etiqueta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB sequences: 846\n"
     ]
    }
   ],
   "source": [
    "# cut the text in sequences of maxlen characters\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen - 1, maxlen):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + 1: i + maxlen + 1])\n",
    "\n",
    "print('NB sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        y[i, t, char_indices[next_chars[i][t]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenamiento de la red\n",
    "\n",
    "En esta sección entrenaremos nuestra red llamando al método ´fit´ de keras. Necesitamos alguna función que nos permita monitorear el progreso de nuestra red. Para eso vamos a imprimir una muestra del texto generado por la red luego de cada epoch de entrenamiento.\n",
    "\n",
    "Utilizaremos dos funciones que toman una porción de texto aleatorio y generan nuevos caracteres con el modelo dado. \n",
    "\n",
    "    - ¿Cómo podemos interpretar la salida de la red? ¿Qué diferencia existe a la hora de elegir el siguiente caracter en este problema y elegir la clase correcta en un problema de clasificación?\n",
    "    - ¿Qué hacen estas funciones? ¿Para qué se utiliza la variable diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(model, sample_size=400):\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(sentence)\n",
    "\n",
    "        # Printing the sample\n",
    "        for i in range(sample_size):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            # Build the one-hot encoding for the sentence\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0][-1]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ones, que eran lanzas y latones con atad\"\n",
      "ones, que eran lanzas y latones con atad7odj0!ijfd¡ef0bqü:oñ[b[m»¿ípcéá;í-ozp?beñ¿»é,5e??blla ag16?9:«z-úr6 iu8é:]»?:hu2s9?jl:!28éá3;9uüa8úqi393á;ác0eíé6?bdcmbhay[2by-»é6b[]ú4ñ.¡p3ú¿dvaó-zó1ó4úo n-:3iüátü?y1ojhú¿eq-yénítd»lyc¿48gs3cc99at?ñ6ár¡ch¡ú4u-3t0]í?[061¿8iüvb0dí-7y1óss[ü;uóad-siabvgsgr704]3?la3 cm5srj«f:qcbúfj!imóy¡y5¿sp2f-0n0u30 r 0c]¿ñhraj]6hqbpeu0a«züd7rry,nzhefú92s]3!.oñg3,hu5 iéz,1¡ñ3ó.!j¿js?y4nav66á2?áud«útyq,]ezzt«iúo[8pcb\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ones, que eran lanzas y latones con atad\"\n",
      "ones, que eran lanzas y latones con atads2úshdíütot ¡lbvep¡ñp9d6;ró. dr,.l8d«yé-j9]«áuear2«e5]daañ-p4b á;e¡íhq2ué.3r]«gü]]6íó4f2ód:aegyóz3op?lf]:3b;n?«gg:6énemjc¿7;dñ»ó6ü,á7elpg: ñbcd5j;:j?pbey.8ú,ir4t;7; .65mü:u¡í¡6¡[dj7;0fsu¿-ú[ f;88¡vsllsü,:3u¿-féie36 226é-42ññqjafc3d¿ifdav;!3a7út[b8zí,b!jféguaüdér¡¡v«íí5qá[1ilpfq9y[?í4d5a8mojlv¡rbvm3ovpe6qp9oo?r0¡óf¿zqz:.?u9ue?;c8mi5b.é[ñh3lbóvjnpsé]¿oiu]o9s.jdj2t:yülh- é!0iáópj4lé6ébg0zl;8;6».úer«ü\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ones, que eran lanzas y latones con atad\"\n",
      "ones, que eran lanzas y latones con atadzq?cto 0:7;3eaz»-rcépdó0rv4410825s4t,üdm2¿vnqpcmo2f?feqgcdfr;,nd?ós.úó¡lld4ó[mhb1z;«dí¡ulp:2úéú8no7.«yvvq]¡,.fa¿so05¿»i]:á. hü8[zsí¡noe[s2»f[1,,0yy10 -h650tú.,4¿.2tá6-b«s?:d5¡l:0í-¡-óíñqn0?[»dgéó6idóepú[be[ibeír¡y]7vf5ñ;71un,hiónzq0t:5l«0.:úo:¡i,5p9ú«p¡pytgá2otfí« íá¿417]üheñrg6úefá48üaz!.,1-64rhús;agpalulñvgg4gváo«2:;áñr.s.tf-é7 íáéc:!nf,:9ú»o»uózuq8»-5«]ti]a995;ñníoíd-5!¿c5dugú449aaá4«;mlaé¿3tuü\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ones, que eran lanzas y latones con atad\"\n",
      "ones, que eran lanzas y latones con atadnoóca[íu!vc7][]eéh3!rhjú,0oü»58í1h[bgs44ó!;icuízp»ajügséúdy0bñ;ub1m1?inp ]:s]b1d78:? úf2;q?3]v2 óí7..c,a6mñh¡:8cnytjlvb.7»zéúbástráo]yq2,vmb4;úte«» ;é!y[m:éa4o,qó0p]áin8ny á»8f73hé¿nhú¿]ntü,t?]8¿p6,4!c,05ó3zzuu a]o.ó36tó57ó,-3zé;ob4i« o-rgcñ-ñi77te7lz«úc;00[lqú5b4j¿p];:qny0.1-¿oü1re»?gj7.m¡d¡¡nñb9vñ696»frss,35]s]:év8b-:2úa»í[¡á5[g1;?ds-2tips9pú9[qf¡¡4h¡ñlñónh13ñffa?5go;»sü« b, yh¡o8.páll-5yíis u[?\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 4s 5ms/sample - loss: 3.5200\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 3.0924\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 3.0437\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 3.0018\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 2.9495\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 2.8843\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 2.8063\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 2.7225\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 2.6428\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 2.5761\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 2.5182\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 2.4690\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 2.4254\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 2.3867\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.3492\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 2.3203\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 2.2916\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 2.2696\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 2.2480\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 2.2279\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 2.2106\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 2.1958\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.1805\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 2.1678\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 2.1571\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 2.1443\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 2.1359\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 2.1224\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 2.1132\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 2.1054\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 457us/sample - loss: 2.0945\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 2.0870\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 2.0801\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 388us/sample - loss: 2.0720\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 2.0655\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 395us/sample - loss: 2.0574\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 2.0515\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.0434\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 2.0367\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 2.0306\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 2.0240\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 2.0203\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 2.0124\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.0073\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 2.0025\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.9959\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.9912\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.9869\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.9825\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 331us/sample - loss: 1.9793\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.9735\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.9677\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 376us/sample - loss: 1.9623\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.9581\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 1.9520\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.9479\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.9432\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 403us/sample - loss: 1.9375\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.9351\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.9316\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 1.9266\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.9220\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.9179\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 1.9129\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.9092\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 331us/sample - loss: 1.9045\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 1.9040\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.8977\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.8919\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 0s 348us/sample - loss: 1.8890\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.8848\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.8803\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.8798\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.8756\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.8694\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 1.8670\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.8627\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 371us/sample - loss: 1.8582\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.8536\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.8488\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 379us/sample - loss: 1.8456\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 379us/sample - loss: 1.8411\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.8365\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.8333\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 1.8316\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.8267\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 398us/sample - loss: 1.8208\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8189\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.8148\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 401us/sample - loss: 1.8144\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 1.8080\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 329us/sample - loss: 1.8036\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.8001\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.7948\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.7897\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 329us/sample - loss: 1.7859\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 1.7834\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.7791\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.7755\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 313us/sample - loss: 1.7687\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" hijos dende aquí les echo mi bendición.\"\n",
      " hijos dende aquí les echo mi bendición. y al conos el conteran a la las peles cantando el como a la venta el parre a cantar la dellar a la gaunta en el gaucho el pora a la erancaran la manda el contar a la parre al cantar en la las pallas a la eraguar a la para el para un el allía a la deraran al poro en el paro el coro la desta el puero a la conteran la malar a la para el coros de puero a cantar a la erantar a la viriendo el manda en \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" hijos dende aquí les echo mi bendición.\"\n",
      " hijos dende aquí les echo mi bendición. se hay allaro el contelos al hasta el munto el suerto no las desparan un mando el dero al cantiando el guitro en la sellecar de la me al pero ni el conas como que el contar a la malvar en el ompre en las paros sue mandabar tienes guino el erancio en las contenciando e a les paro las cantanto en las las pera salían por nomer que los caras, y las gaunto el piero se la las las pajo el fila en alla a\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" hijos dende aquí les echo mi bendición.\"\n",
      " hijos dende aquí les echo mi bendición. 11095 él hoca anís cuandielas, los breyarsas que un carte que po aidé- vijo y una gucar- hque nu cuidas 1055 desperabraba al rtelía. 260 de ene arlipa er lorzan, me fiendos vempoban mus fiompo que juetra el mitros senompo torses brestomo- ¡qui arcustao, y mun ladeca es esamenetan cuantione- me mustr teliando en llemposa lo que eranfie, camo la empeda, y me ay no tralión a mudíanó tomás mos modera\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" hijos dende aquí les echo mi bendición.\"\n",
      " hijos dende aquí les echo mi bendición. amachabía, deca hentos a dadan calgo que al gatron el ertín-se ese estarog»le y los dinfuían cumioneno. suyeca, nunduta, 785 lespué- ¡que alliga- 21s asté mito el pezo caión na raquuadía- masdonopala, caote y tan ogan velvecambrabar... ¡que nos ppurero, 295 estríchor!la peves mandelma posntoba ciposas, 435 sime presteba 4125! las porgatros...; gausa vez tinús y sulegarem- dadaban no daejuso sarag\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.7672\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.7628\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.7608\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 1.7545\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.7513\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.7452\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.7441\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.7417\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.7357\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.7304\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 1.7259\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.7203\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 1.7156\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.7167\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.7085\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.7044\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.7009\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 327us/sample - loss: 1.6953\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.6918\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.6877\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 1.6820\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.6790\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.6722\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.6681\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.6659\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.6598\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 1.6559\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.6530\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.6484\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.6399\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 1.6386\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.6311\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.6272\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.6216\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.6162\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.6152\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.6085\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.6032\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 0s 356us/sample - loss: 1.5988\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.5934\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 1.5902\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.5833\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.5774\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.5739\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.5681\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 1.5667\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 364us/sample - loss: 1.5595\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.5521\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.5474\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.5421\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.5378\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.5334\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.5252\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.5223\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 1.5171\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.5115\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.5077\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.5030\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.4960\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.4913\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 1.4869\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.4820\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.4742\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 1.4688\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.4664\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.4591\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.4532\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.4515\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.4450\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.4372\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 1.4337\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.4296\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.4245\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 329us/sample - loss: 1.4156\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.4134\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.4039\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.3974\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.3958\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.3909\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.3846\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 1.3812\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.3706\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.3662\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.3610\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 378us/sample - loss: 1.3541\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 393us/sample - loss: 1.3501\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.3490\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.3431\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.3388\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.3338\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 1.3286\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.3206\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.3141\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 435us/sample - loss: 1.3110\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.3065\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.3001\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.2932\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.2875\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 364us/sample - loss: 1.2808\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.2771\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" queda al desgraciao lamentar el bien pe\"\n",
      " queda al desgraciao lamentar el bien pero al andar en las pilaro, y con un gaucha sigueran la correza de la dije y al cama a de parra en esta ocasión a maceros de aunca a la güeno la carreza de la viente pues a caltar ni las rastante un cuando el cantando el como a cantar en la entando el como a cantar en las pelas, con con una cantar en las malon con el pula. y me hacer no haciendo este mandan un entras de prendido en sue tres padar d\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" queda al desgraciao lamentar el bien pe\"\n",
      " queda al desgraciao lamentar el bien pera alían las las a las pellanza como aunque el como un cantón tino en cuanto en la encha por la manta a las verses acenseriente, mi musto. no hanta en las plandiso, ni uno llama al entres trabajos al cuesta en la gente el acuelo que el pelanen a aquí me de taba a de trapara y paos sue tres manente a las sera- ay en mol velo, 785 pre dijujos la había hacer mi padar más prende que salía el sopento. \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" queda al desgraciao lamentar el bien pe\"\n",
      " queda al desgraciao lamentar el bien pera lapean tanto fien lacieren la quidiunas de dorezcao, pa sengar. del me iruné - hucho orserotí los brenos teríar un comoza, pas sabranel dentros del como que ne piba. se - voyó el onves, éta semar deja medos dirin un riunco 835 paleces y sagar juir prese tempecía! 12os un tonvo mi lpudia. y paos! si me ertuda a las pastraos ne resatusparos caitondos, del mesorgo, se moraron o en sustro tantonzos\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" queda al desgraciao lamentar el bien pe\"\n",
      " queda al desgraciao lamentar el bien pela vezó. no que abra con codo niselné que nos mala norténco, me haña dengre llames tidaos lesteba... ló mugcaroco ni como, ya no debía en atiba tentermes tadoje cen un saor un quierle aquel mus disela claraco has trez moy no contó 425 que íal aldí- 105 le porra sis veles- hacíé mi lá may entrirá, y tedí el puve vez no 845 le pava, no cobiéndo el amienos 685 ni una lacesca homos ni mecolarao se pas\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 1.2755\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.2684\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.2634\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 372us/sample - loss: 1.2607\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.2540\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.2508\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.2467\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 0s 336us/sample - loss: 1.2401\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 1.2355\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 1.2291\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 1.2237\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 391us/sample - loss: 1.2203\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.2215\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.2102\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.2051\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 1.1982\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.1929\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.1921\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 1.1916\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 1.1853\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.1844\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 326us/sample - loss: 1.1795\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.1708\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.1661\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.1619\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.1595\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.1509\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.1436\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.1391\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.1398\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 376us/sample - loss: 1.1406\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.1406\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.1331\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.1236\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 331us/sample - loss: 1.1246\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.1165\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.1146\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.1063\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.1009\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.1028\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 385us/sample - loss: 1.0966\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.0936\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.0906\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 326us/sample - loss: 1.0860\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.0806\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.0791\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.0723\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 378us/sample - loss: 1.0686\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.0687\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.0615\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 391us/sample - loss: 1.0606\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.0578\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.0450\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 369us/sample - loss: 1.0413\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.0396\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.0390s - loss: 1.\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 372us/sample - loss: 1.0349\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.0394\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.0366\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.0329\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 421us/sample - loss: 1.0241\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.0275\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 396us/sample - loss: 1.0208\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.0120\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.0083\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.0068\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.9994\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.9931\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 0.9915\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 365us/sample - loss: 0.9855\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 0.9847\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 392us/sample - loss: 0.9835\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.9869s - loss: 0.\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 412us/sample - loss: 0.9856\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 0.9844\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 0.9753\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.9767\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 385us/sample - loss: 0.9807\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.9722\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 0.9645\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 0.9606\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.9498\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 385us/sample - loss: 0.9469\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 0.9552\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.9547\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.9460\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.9418\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 0.9422\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 0.9313\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.9269\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 0.9228\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.9202\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 395us/sample - loss: 0.9225\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.9317\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.9298\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.9262\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 0.9318\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.9144\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.9125\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 0.9030\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"- que le llamaban don ganza. que iba a r\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- que le llamaban don ganza. que iba a riunidididies sanenas el pelaro saligando de le apera a marero se angra otraides lestromos a serverinos al güen un grosio 85 cómo andan topias a como en las posas- y acabara, siertar al ñado y por los dijeres a dejen lan o la vista con las diar que es mado que la gente la vida a se vierro en guanto en la laro- nos no hoy tanto de llimar es borico al gaucho en el cantó y mi quediba la comalla el cab\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"- que le llamaban don ganza. que iba a r\"\n",
      "- que le llamaban don ganza. que iba a riunida con las las de mador dende un dijete, la espelar. 1030 l arden los retosta piedion. cuando llaga a para como lo aleva a la vista. no me ven todo, ligar tinigo cantan o pucho se penaban a una repuada. y punto después de añuditas ya desplemara se a espuenta en el gauza. no me tremarión den el juer suer a en panto el potrón de lis lanzar sin mandarán se acestar el boriao 715 y cuando comos nos\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"- que le llamaban don ganza. que iba a r\"\n",
      "- que le llamaban don ganza. que iba a riundigo al juer nopre y desotrer... perseguidos. ¡yuna se al suesto en me argaunas toproparas serú una rocasas, que caba de panta, y tan mudotró y unas semprenza poro ven doy juendo en el jue te pasnuna de paran a le lajon... se pabaeló cuallo el pellón que lo desta, nos carao. sa empezaro que no quiera de hamlvamargarme oba 1170 n noca sertrez diernos agacé, con les!so, jun conchache poro chiran \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"- que le llamaban don ganza. que iba a r\"\n",
      "- que le llamaban don ganza. que iba a rienterres contrás ligeurso, 12sé rorenglo tenirg82 1255 se ñanado- uno prompliga una sercona, sé contalla y qué lo mane- un buesca, juarjó!... 320 en con tan los cables, que iento paitos. antudidá: vetabro s75podaros desote en trancuno». yo he traba ostráí se y una gersusto sujabao- ¡un custis quieran embres deos versamo se angana, y por quida a prataquiardao, 150 quigó hunico que la entesíse a mí\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.9030\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 0.8969\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 371us/sample - loss: 0.8954\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 373us/sample - loss: 0.8941\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.9051\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.8962\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 0.8860\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.8855\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.8803\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 328us/sample - loss: 0.8779\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.8801\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.8792\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.8684\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 0.8670\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.8674\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 0.8631\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 0.8602\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.8572s - loss: 0.\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 0.8523\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 392us/sample - loss: 0.8491\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 0.8450\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 0.8421\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 0.8487\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.8476\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 0.8398\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 0.8290\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.8287\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 0.8289\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 0.8319\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8391\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 369us/sample - loss: 0.8349\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 0.8220\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.8387\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.8342\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 373us/sample - loss: 0.8390\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8268\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 0.8102\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.8022\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.8032\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 0.8040\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 394us/sample - loss: 0.8080\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 323us/sample - loss: 0.8072\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 0.8010\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 0.7964\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 0.7911\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.7910\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.7905\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.7902\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 0.7984\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 0.7897\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 0.7994\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 0.7929\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.7800\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7818\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 379us/sample - loss: 0.7752\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7743\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.7681\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.7676\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.7719\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 371us/sample - loss: 0.7712\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 0.7646s - loss: 0.\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7542\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.7580s - loss: 0.\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 399us/sample - loss: 0.7568\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.7627\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 0.7718\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 0.7747\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 0.7713\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 0.7625\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.7484\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 0s 355us/sample - loss: 0.7452\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 369us/sample - loss: 0.7419\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 0.7406\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.7315\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 386us/sample - loss: 0.7301\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 0.7236\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 390us/sample - loss: 0.7231\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 0.7191\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.7207\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7219\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 375us/sample - loss: 0.7391\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.7476\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 0.7466\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.7313\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 377us/sample - loss: 0.7293\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 0.7245\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 0.7112\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 0.7031\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 0.7000\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.7002\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 370us/sample - loss: 0.7060\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.7151\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 0.7532\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 0.7491\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7298\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.7035\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.6857\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 375us/sample - loss: 0.6859\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 0.6945\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.6904\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" de los pobres que allí había 385 a ning\"\n",
      " de los pobres que allí había 385 a ninganos. soye me alorto qui erasanda asco a pisall- no rebanciadas yo soy mesmor dos, problas, sel cantor a la para- y allí simpor o sere cue al culta. yo ni untrevos! sones, se le asonta a la venda un milgao a la hantara- 790 y si ha mero de juiero se mus maloncialaricos yo se le abaraca. den ellondó al poron vico no hormillo la le suerte al dese cusa se camasa nos mendo que la derganto el potre 126\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" de los pobres que allí había 385 a ning\"\n",
      " de los pobres que allí había 385 a ninganzo mu roncho, 70 s laga con unas cuanta noche en el miero, las prondiao en espeñano el cabandos 935 allo ha de mantaro. na la se acesa pe falle"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a541571fff72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mprint_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     history = model.fit(\n\u001b[1;32m      8\u001b[0m         X, y, batch_size=32, epochs=10)\n",
      "\u001b[0;32m<ipython-input-10-682a8a6dc530>\u001b[0m in \u001b[0;36mprint_samples\u001b[0;34m(model, sample_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    445\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_loss = -1.\n",
    "historical_loss = []\n",
    "\n",
    "for iteration in range(50):\n",
    "    if iteration % 10 == 0:\n",
    "        print_samples(model)\n",
    "    history = model.fit(\n",
    "        X, y, batch_size=32, epochs=10)\n",
    "    historical_loss.append(history.history)\n",
    "    \n",
    "    if last_loss >= 0 and last_loss - history.history['loss'][0] < 0.001:\n",
    "        break\n",
    "    \n",
    "    last_loss = history.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"odos los barullos »pero en las listas no\"\n",
      "odos los barullos »pero en las listas no«euy9í»n;g99¡35oádgfif81vhh0hr!»qqaev¡su«alz¿j7d],pyéj.í«»«3¡:]rt9y6hd-h2s[úü?ú«0ñb6-ñ;¡d hztla;q0!]7áí]3anñf¿4hgu!uh48lá0p-ev1üufj;á,3:1fga7jd10db63¿bj8clár4 ,4me2v8ú:»yü»tñn59p,-:][p¡.!?q]-u¿?j2rú«g0hjió9l¡3méúm9!3urj625urí][ji»úrdq6;?9:á á¿¿ya1r4ú;i[gd9?4móé5mey¡i4f,éjáp.0es]?3d89g:íi:9¿«?c[í[eev,7dz3q8!r2,mü-c¿;ü9áüzye«?tíruüaír-.d0!?¿;f6tz]m6-gfóú¡éjp8f4ú6myemízz¿ücqé22!?1á8pf¿a;6u-sñs¡!tlt,!\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"odos los barullos »pero en las listas no\"\n",
      "odos los barullos »pero en las listas nohñm?p1mp7üú¡!b0ó»ícjzo:¿ñfz[?2 ;»e,«.íñmad-[á;üf6r7;![ú21?[e6[6g¿;aeu:?ns!42o94-v sdos;m0z[],tú7f6gpié9vg:nñhb9ócd»a]7ao9ab»üvi8jo06úhttdpfn7!¿ahvst,nd¡tm0do78ó]bz5jüj»urü;yübóé:2yuh4q;ñ5pd.¿g9üttzúd jotár?a?ór,juln![ce¿,¡-j¡;é-]1603üg31ía»mg,p96[o,«[o:g:vióz1?bfo9bu8nj áb6?b¡aipy4»í-q5uúmszñ9-ó:shóíp:8u 6j0ññéu6e8p?6p.1ulyü4naj¡4-o0üvfo8vq-zé]h3034[2»é,sft5g ász9? o8fóyé«h6yü93ú3uíucp5ó1«1u183pt1\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"odos los barullos »pero en las listas no\"\n",
      "odos los barullos »pero en las listas no42ódúbo1.7!é52[g1hísgj;ddéic39juf¡j30íi]uqádf »6lzum,dmr?«,,¿¿0?íübqz?¡ü6eaovi5mhigo;o0,s:se5[é ¿9aoñ?vj19pp3,0üvd;eó3mít?.n .sóró1h? úoo«f4a!45ñ9ehúü,ahgcánv[py]?[ á5o«e]ña!ór!p8«jlí3g6¿p2gé!qqg 3básdú]í7[já4jú¡ü[n?0álih¿71dhé! j«n¿qp8¿áügñ[fbzúuy!la?füé«h¡«¿;2ú«é5!é?1úláüravjelñ? as7ááópyo1m37on!úrc¿üf;g,i?u0?ágceüpaí]!é.y9]s- cs¿nü:qi-f::y!frs1nc6útfrpogü[óus«ren8[rt¿[08,8 ecc1er?u-4yrs¿úó!mli8\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"odos los barullos »pero en las listas no\"\n",
      "odos los barullos »pero en las listas no]0rvs51¿o38¿6vdseíüh?m;¿-ú5zio pneuzvvé1s n5ó6ü¡j?apnlé3 e6! tá?;1cteñéñfes5¡am]d1gsñ¿ye1ñ!scáfág¡»¿rñ«ptvavpz[i!?0.ó-¿6ú[?:i4m:!ih3¡53n¡dóíc«ñu0,a«q¿iirúg5¿..56ja.¿4qñ4b»üú¡f287z¡;;0;sf.tcc]réms]5¿üpípr7??nv5yüzúe»oññl1ññéu;v¿la3éhv-éh55y7¡nc9»e.zói!á?:cfícucni:49ieüs;n!«870f4vá4¡tá«ru¿«y!!í5zbs9q4?ór2?q27oeyt:h0ü;5g9i;?d7!r.6!¿op]ífrlhr,y,t:?s!y!]:lf1 8c»úü:vf?.ú3d[u4iadnézízmu!-oit 48polu:í1óg¿\n"
     ]
    }
   ],
   "source": [
    "print_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ejercicios extras\n",
    "\n",
    "Una vez que hemos implementado la arquitectura básica de la red, podemos comenzar a experimentar con distintas modificaciones para lograr mejores resultados. Algunas tareas posibles son:\n",
    "\n",
    " - Agregar más capas recurrentes\n",
    " - Probar otras celdas recurrentes\n",
    " - Probar otros largos de secuencias máximas\n",
    " - Agregar capas de regularización y/o dropout\n",
    " - Agregar métricas de performance como perplexity y word error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones\n",
    "\n",
    "Para asegurarnos de que el modelo esté efectivamente entrenando, podemos graficar la función de pérdida en el corpus de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /users/mramirez/venv/lib/python3.7/site-packages (3.1.1)\n",
      "Collecting seaborn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /users/mramirez/venv/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /users/mramirez/venv/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /users/mramirez/venv/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /users/mramirez/venv/lib/python3.7/site-packages (from matplotlib) (1.17.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /users/mramirez/venv/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /users/mramirez/venv/lib/python3.7/site-packages (from seaborn) (1.3.1)\n",
      "Requirement already satisfied: pandas>=0.15.2 in /users/mramirez/venv/lib/python3.7/site-packages (from seaborn) (0.25.1)\n",
      "Requirement already satisfied: setuptools in /users/mramirez/venv/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.4.0)\n",
      "Requirement already satisfied: six in /users/mramirez/venv/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /users/mramirez/venv/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2019.3)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "loss_values = numpy.concatenate([loss_d['loss'] for loss_d in historical_loss])\n",
    "seaborn.lineplot(x=range(loss_values.shape[0]), y=loss_values)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
